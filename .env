# .env.example
# Copy to .env and update values. This file shows environment variables used by the POC.

LLM_BACKEND=local

# Primary: local LLM server (Ollama / TGI / dev server). Expected to accept OpenAI-like /v1/chat/completions.
# Example Ollama: http://localhost:11434
# Example TGI (text-generation-inference behind proxy): http://localhost:8080
LOCAL_LLM_URL=http://localhost:11434

# Model name to query on the local server (optional; included in payload by utils).
LOCAL_LLM_MODEL=codellama:7b

# Fallback: OpenAI API key (if you don't have a local LLM running).
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_DEPLOYMENT=
AZURE_OPENAI_API_VERSION=2025-01-01-preview
AZURE_MODEL=gpt-4o

# Timeout for local LLM calls (seconds)
LOCAL_LLM_TIMEOUT=300

LLM_TEMPERATURE=0.0

# Output folders (optional override)
OUTPUT_DIR=output
UNIFIED_DIR=unified_specs
SUMMARIES_DIR=summaries

# Streamlit port (optional)
STREAMLIT_PORT=8501

# For Testing
TEST_JAVA_FILE_LOC=data/javadoc
TEST_JAVA_TO_PYTHON_FILE_LOC=data/python

PROMPT_FOLDER=data/prompts

# Token limit for prompts (approximate)
LLM_TOKEN_LIMIT=2000

OUTPUT_JAVA_PARSED_FOLDER=output/parsed_results
TEST_JAVA_FILE=data/javacode/HelloWorld.java
LLM_DOC_INPUT_FOLDER=output/llm_doc/input
LLM_DOC_OUTPUT_FOLDER=output/llm_doc/output
LLM_PYTHON_INPUT_FOLDER=output/llm_python/input
LLM_PYTHON_OUTPUT_FOLDER=output/llm_python/output