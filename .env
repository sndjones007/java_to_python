# .env.example
# Copy to .env and update values. This file shows environment variables used by the POC.

LLM_BACKEND=local

# Primary: local LLM server (Ollama / TGI / dev server). Expected to accept OpenAI-like /v1/chat/completions.
# Example Ollama: http://localhost:11434
# Example TGI (text-generation-inference behind proxy): http://localhost:8080
LOCAL_LLM_URL=http://localhost:11434

# Model name to query on the local server (optional; included in payload by utils).
LOCAL_LLM_MODEL=codellama:7b

# Fallback: OpenAI API key (if you don't have a local LLM running).
OPENAI_API_KEY=

# Timeout for local LLM calls (seconds)
LOCAL_LLM_TIMEOUT=300

LLM_TEMPERATURE=0.0

# Output folders (optional override)
OUTPUT_DIR=output
UNIFIED_DIR=unified_specs
SUMMARIES_DIR=summaries

# Streamlit port (optional)
STREAMLIT_PORT=8501

# For Testing
TEST_JAVA_FILE_LOC=data/javadoc
TEST_JAVA_TO_PYTHON_FILE_LOC=data/python

PROMPT_FOLDER=data/prompts

# Token limit for prompts (approximate)
LLM_TOKEN_LIMIT=2000
